import requests
import json
import os
import re
import time
from datetime import datetime

# -- C·∫•u h√¨nh --
API_KEY = os.environ["GEMINI_API_KEY"]
MODEL = "gemini-1.5-flash"
API_URL = f"https://generativelanguage.googleapis.com/v1/models/{MODEL}:generateContent?key={API_KEY}"

ALBUMS = [
    "https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=MzU5NjU1NjY1Mw==&album_id=3447004682407854082&f=json",
    "https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=MzkyMjc1NzEzOA==&album_id=3646379824391471108&f=json",
    "https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=MzI1MDQ1MjUxNw==&album_id=3664489989179457545&f=json"
]

# -- B·∫£ng t·ª´ chuy√™n ng√†nh --
GLOSSARY = {
    "ÊµÅ": "l·ªëi ch∆°i",
    "Êú®Ê°©": "c·ªçc g·ªó",
    "Ê≤ßÊæú": "Th∆∞∆°ng Lan",
    "ÊΩÆÂÖâ": "Tri·ªÅu Quang",
    "ÁéÑÊú∫": "Huy·ªÅn C∆°",
    "ÈæôÂêü": "Long Ng√¢m",
    "Á•ûÁõ∏": "Th·∫ßn T∆∞∆°ng",
    "Ë°ÄÊ≤≥": "Huy·∫øt H√†",
    "Á¢éÊ¢¶": "To√°i M·ªông",
    "Á¥†ÈóÆ": "T·ªë V·∫•n",
    "‰πùÁÅµ": "C·ª≠u Linh",
    "ÈìÅË°£": "Thi·∫øt Y"
}

def cleanup_translation(text):
    text = re.sub(r"\*\*.*?\*\*", "", text)
    text = re.sub(r"\n+", "\n", text)
    text = re.sub(r"\s{2,}", " ", text)
    return text.strip()

def fix_terms(text):
    for zh, vi in GLOSSARY.items():
        text = text.replace(zh, vi)
    return text

def batch_translate_zh_to_vi(titles, retries=3, delay=10):
    joined_titles = "\n".join(titles)
    prompt = (
        "B·∫°n l√† m·ªôt chuy√™n gia d·ªãch thu·∫≠t ti·∫øng Trung - Vi·ªát, c√≥ hi·ªÉu bi·∫øt s√¢u s·∫Øc v·ªÅ game mobile Trung Qu·ªëc, ƒë·∫∑c bi·ªát l√† 'Ngh·ªãch Th·ªßy H√†n Mobile'.\n"
        "H√£y d·ªãch t·∫•t c·∫£ c√°c ti√™u ƒë·ªÅ sau sang **ti·∫øng Vi·ªát t·ª± nhi√™n, s√∫c t√≠ch, ƒë√∫ng vƒÉn phong gi·ªõi game th·ªß Vi·ªát**, mang m√†u s·∫Øc h·∫•p d·∫´n, ∆∞u ti√™n gi·ªØ nguy√™n c√°c thu·∫≠t ng·ªØ k·ªπ thu·∫≠t, t√™n v·∫≠t ph·∫©m, v√† c·∫•u tr√∫c ti√™u ƒë·ªÅ g·ªëc.\n\n"
        "‚ö†Ô∏è Quy t·∫Øc d·ªãch:\n"
        "- Gi·ªØ nguy√™n c√°c c·ª•m s·ªë (nh∆∞ 10W, 288).\n"
        "- Gi·ªØ nguy√™n t√™n k·ªπ nƒÉng, v≈© kh√≠, t√≠nh nƒÉng trong d·∫•u [] ho·∫∑c „Äê„Äë.\n"
        "- ∆Øu ti√™n t·ª´ ng·ªØ ph·ªï bi·∫øn trong c·ªông ƒë·ªìng game nh∆∞: 'build', 'ph·ªëi ƒë·ªì', 'ƒë·∫≠p ƒë·ªì', 'l·ªô tr√¨nh', 'trang b·ªã x·ªãn', 'ngo·∫°i h√¨nh ƒë·ªânh', 'top server'...\n"
        "- C√°c t·ª´ c·ªë ƒë·ªãnh ph·∫£i d·ªãch ƒë√∫ng theo b·∫£ng sau:\n"
        "- ÊµÅ = l·ªëi ch∆°i\n"
        "- Êú®Ê°© = c·ªçc g·ªó\n"
        "- Ê≤ßÊæú = Th∆∞∆°ng Lan\n"
        "- ÊΩÆÂÖâ = Tri·ªÅu Quang\n"
        "- ÁéÑÊú∫ = Huy·ªÅn C∆°\n"
        "- ÈæôÂêü = Long Ng√¢m\n"
        "- Á•ûÁõ∏ = Th·∫ßn T∆∞∆°ng\n"
        "- Ë°ÄÊ≤≥ = Huy·∫øt H√†\n"
        "- Á¢éÊ¢¶ = To√°i M·ªông\n"
        "- Á¥†ÈóÆ = T·ªë V·∫•n\n"
        "- ‰πùÁÅµ = C·ª≠u Linh\n"
        "- ÈìÅË°£ = Thi·∫øt Y\n\n"
        "üö´ Kh√¥ng ƒë∆∞·ª£c th√™m b·∫•t k·ª≥ ghi ch√∫, s·ªë th·ª© t·ª±, ho·∫∑c ph·∫ßn m·ªü ƒë·∫ßu. Ch·ªâ d·ªãch t·ª´ng d√≤ng, gi·ªØ nguy√™n th·ª© t·ª± g·ªëc.\n\n"
        + joined_titles
    )

    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [
            {"parts": [{"text": prompt}]}
        ]
    }

    for attempt in range(retries):
        response = requests.post(API_URL, headers=headers, json=payload)
        if response.status_code == 200:
            result = response.json()
            raw_text = result["candidates"][0]["content"]["parts"][0]["text"]
            clean_text = cleanup_translation(raw_text)
            lines = [fix_terms(line.strip()) for line in clean_text.split("\n") if line.strip()]
            return lines
        elif response.status_code == 503:
            print(f"‚ö†Ô∏è M√¥ h√¨nh qu√° t·∫£i. Th·ª≠ l·∫°i l·∫ßn {attempt + 1}/{retries} sau {delay}s...")
            time.sleep(delay)
        else:
            print("‚ùå L·ªói d·ªãch:", response.status_code, response.text)
            return titles

    print("‚ùå Th·ª≠ l·∫°i nhi·ªÅu l·∫ßn nh∆∞ng v·∫´n l·ªói. B·ªè qua d·ªãch.")
    return titles

def fetch_articles(url):
    print("üîç ƒêang l·∫•y d·ªØ li·ªáu t·ª´ album...")
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json, text/plain, */*",
        "Referer": "https://mp.weixin.qq.com/",
        "X-Requested-With": "XMLHttpRequest"
    }
    resp = requests.get(url, headers=headers)
    data = resp.json()

    articles_raw = data.get("getalbum_resp", {}).get("article_list", [])
    items = []

    weekdays_vi = [
        "Th·ª© Hai", "Th·ª© Ba", "Th·ª© T∆∞", "Th·ª© NƒÉm",
        "Th·ª© S√°u", "Th·ª© B·∫£y", "Ch·ªß Nh·∫≠t"
    ]

    for art in articles_raw:
        title = art["title"]
        url = art["url"]
        cover = art.get("cover_img_1_1") or art.get("cover") or ""
        timestamp = int(art.get("create_time", 0))
        dt = datetime.utcfromtimestamp(timestamp)
        weekday = weekdays_vi[dt.weekday()]
        date_str = f"{dt.strftime('%H:%M')} - {weekday}, {dt.strftime('%d/%m')}"

        items.append({
            "title": title,
            "url": url,
            "cover_img": cover,
            "timestamp": timestamp,
            "date": date_str
        })

    print(f"‚úÖ {len(items)} b√†i vi·∫øt")
    return items

from bs4 import BeautifulSoup

def fetch_article_content(url):
    """
    L·∫•y n·ªôi dung text v√† h√¨nh ·∫£nh t·ª´ m·ªôt b√†i vi·∫øt chi ti·∫øt tr√™n WeChat.
    Tr·∫£ v·ªÅ dict: {"content_text": ..., "images": [list link ·∫£nh], "content_html": ...}
    """
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "text/html,application/xhtml+xml,application/xml",
        "Referer": "https://mp.weixin.qq.com/",
    }
    resp = requests.get(url, headers=headers)
    resp.encoding = resp.apparent_encoding
    soup = BeautifulSoup(resp.text, "html.parser")
    # N·ªôi dung ch√≠nh n·∫±m trong div id="js_content"
    content_div = soup.find("div", id="js_content")
    if not content_div:
        return {"content_text": "", "images": [], "content_html": ""}
    # L·∫•y text
    content_text = content_div.get_text("\n", strip=True)
    # L·∫•y html
    content_html = str(content_div)
    # L·∫•y link ·∫£nh
    images = [img["data-src"] for img in content_div.find_all("img", attrs={"data-src": True})]
    return {"content_text": content_text, "images": images, "content_html": content_html}

def fetch_all_albums(album_urls):
    all_articles = []
    for url in album_urls:
        articles = fetch_articles(url)
        top_4 = sorted(articles, key=lambda x: x["timestamp"], reverse=True)[:4]
        all_articles.extend(top_4)
    sorted_articles = sorted(all_articles, key=lambda x: x["timestamp"], reverse=True)
    return sorted_articles

# -- MAIN --
if __name__ == "__main__":
    articles = fetch_all_albums(ALBUMS)

    news_list = []
    for i, article in enumerate(articles):
        print(f"\nüîó ƒêang l·∫•y n·ªôi dung b√†i vi·∫øt {i+1}/{len(articles)}: {article['title']}")
        content_data = fetch_article_content(article["url"])
        content_zh = content_data["content_text"]
        images = content_data["images"]
        # D·ªãch ti√™u ƒë·ªÅ v√† n·ªôi dung
        print("üåê ƒêang d·ªãch ti√™u ƒë·ªÅ + n·ªôi dung...")
        to_translate = [article["title"], content_zh]
        vi_results = batch_translate_zh_to_vi(to_translate)
        vi_title = vi_results[0] if len(vi_results) > 0 else article["title"]
        vi_content = vi_results[1] if len(vi_results) > 1 else content_zh
        if re.search(r'[\u4e00-\u9fff]', vi_title) or re.search(r'[\u4e00-\u9fff]', vi_content):
            print(f"‚ö†Ô∏è B√†i {i+1}: D·ªãch ch∆∞a ho√†n ch·ªânh!")
        print(f"‚û°Ô∏è {vi_title}")
        news_list.append({
            "title_zh": article["title"],
            "title_vi": vi_title,
            "content_zh": content_zh,
            "content_vi": vi_content,
            "url": article["url"],
            "cover_img": article["cover_img"],
            "images": images,
            "date": article["date"]
        })

    with open("news.json", "w", encoding="utf-8") as f:
        json.dump(news_list, f, ensure_ascii=False, indent=2)

    print("\nüéâ Ho√†n t·∫•t! ƒê√£ t·∫°o file news.json v·ªõi n·ªôi dung v√† h√¨nh ·∫£nh.")

    # Render lu√¥n news_full.html
    def render_news_html(news_json_path, output_html_path):
        with open(news_json_path, "r", encoding="utf-8") as f:
            news_list = json.load(f)

        html = [
            "<html>",
            "<head>",
            "<meta charset='utf-8'>",
            "<title>Tin t·ª©c d·ªãch ƒë·∫ßy ƒë·ªß</title>",
            "<style>body{font-family:sans-serif;max-width:800px;margin:auto;background:#f7f7f7;}h1,h2{color:#2b4f81;}article{background:#fff;padding:24px 32px;margin:32px 0;border-radius:10px;box-shadow:0 2px 8px #0001;}img{max-width:100%;margin:16px 0;border-radius:6px;}</style>",
            "</head>",
            "<body>",
            "<h1>Tin t·ª©c d·ªãch ƒë·∫ßy ƒë·ªß</h1>"
        ]
        for news in news_list:
            html.append("<article>")
            html.append(f"<h2>{news['title_vi']}</h2>")
            html.append(f"<div style='color:#888;font-size:14px;margin-bottom:8px'>{news['date']}</div>")
            if news.get("cover_img"):
                html.append(f"<img src='{news['cover_img']}' alt='cover' loading='lazy'>")
            # N·ªôi dung b√†i vi·∫øt
            html.append("<div style='white-space:pre-line;font-size:17px;line-height:1.7;margin:18px 0 0 0'>")
            html.append(news['content_vi'].replace("\n", "<br>"))
            html.append("</div>")
            # ·∫¢nh trong b√†i
            if news.get("images"):
                for img_url in news["images"]:
                    html.append(f"<img src='{img_url}' alt='·∫£nh b√†i vi·∫øt' loading='lazy'>")
            html.append("</article>")
        html.append("</body></html>")
        with open(output_html_path, "w", encoding="utf-8") as f:
            f.write("\n".join(html))
        print(f"‚úÖ ƒê√£ t·∫°o {output_html_path}")

    render_news_html("news.json", "news_full.html")
