import requests
import json
import os
import re
import time
from datetime import datetime
from bs4 import BeautifulSoup

# -- Cáº¥u hÃ¬nh --
API_KEY = os.environ["GEMINI_API_KEY"]
MODEL = "gemini-1.5-flash"
API_URL = f"https://generativelanguage.googleapis.com/v1/models/{MODEL}:generateContent?key={API_KEY}"

ALBUMS = [
    "https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=MzU5NjU1NjY1Mw==&album_id=3447004682407854082&f=json",
    "https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=MzkyMjc1NzEzOA==&album_id=3646379824391471108&f=json",
    "https://mp.weixin.qq.com/mp/appmsgalbum?action=getalbum&__biz=MzI1MDQ1MjUxNw==&album_id=3664489989179457545&f=json"
]

# -- Báº£ng tá»« chuyÃªn ngÃ nh --
GLOSSARY = {
    "æµ": "lá»‘i chÆ¡i",
    "æœ¨æ¡©": "cá»c gá»—",
    "æ²§æ¾œ": "ThÆ°Æ¡ng Lan",
    "æ½®å…‰": "Triá»u Quang",
    "ç„æœº": "Huyá»n CÆ¡",
    "é¾™åŸ": "Long NgÃ¢m",
    "ç¥ç›¸": "Tháº§n TÆ°Æ¡ng",
    "è¡€æ²³": "Huyáº¿t HÃ ",
    "ç¢æ¢¦": "ToÃ¡i Má»™ng",
    "ç´ é—®": "Tá»‘ Váº¥n",
    "ä¹çµ": "Cá»­u Linh",
    "é“è¡£": "Thiáº¿t Y"
}

def cleanup_translation(text):
    text = re.sub(r"\*\*.*?\*\*", "", text)
    text = re.sub(r"\n+", "\n", text)
    text = re.sub(r"\s{2,}", " ", text)
    return text.strip()

def fix_terms(text):
    for zh, vi in GLOSSARY.items():
        text = text.replace(zh, vi)
    return text

def translate_plain_text_zh_to_vi(text, retries=3, delay=25): # TÄƒng thá»i gian chá» khi retry
    """Dá»‹ch má»™t Ä‘oáº¡n vÄƒn báº£n thuáº§n tÃºy, vá»›i cÆ¡ cháº¿ thá»­ láº¡i."""
    if not text or not text.strip():
        return ""

    headers = {"Content-Type": "application/json"}
    prompt = (
        "Báº¡n lÃ  má»™t chuyÃªn gia dá»‹ch thuáº­t tiáº¿ng Trung - Viá»‡t, chuyÃªn vá» game 'Nghá»‹ch Thá»§y HÃ n Mobile'.\n"
        "HÃ£y dá»‹ch ná»™i dung sau sang tiáº¿ng Viá»‡t má»™t cÃ¡ch tá»± nhiÃªn, chÃ­nh xÃ¡c, giá»¯ nguyÃªn cÃ¡c dáº¥u xuá»‘ng dÃ²ng.\n"
        "KhÃ´ng thÃªm báº¥t ká»³ bÃ¬nh luáº­n, ghi chÃº hay lá»i chÃ o nÃ o.\n\n"
        f"Ná»™i dung cáº§n dá»‹ch:\n{text}"
    )
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0.4}
    }

    for attempt in range(retries):
        try:
            response = requests.post(API_URL, headers=headers, json=payload, timeout=120)
            if response.status_code == 200:
                result = response.json()
                translated_text = result["candidates"][0]["content"]["parts"][0]["text"]
                print("    âœ… Dá»‹ch vÄƒn báº£n thÃ nh cÃ´ng.")
                return fix_terms(translated_text)
            elif response.status_code in [429, 503]:
                wait_time = delay * (attempt + 1)
                print(f"    âš ï¸ Lá»—i {response.status_code} (QuÃ¡ táº£i/Giá»›i háº¡n). Thá»­ láº¡i sau {wait_time}s...")
                time.sleep(wait_time)
            else:
                print(f"    âŒ Lá»—i dá»‹ch: {response.status_code} - {response.text}")
                return ""
        except requests.exceptions.RequestException as e:
            print(f"    âŒ Lá»—i máº¡ng khi dá»‹ch: {e}. Thá»­ láº¡i sau {delay}s...")
            time.sleep(delay)

    print("    âŒ Thá»­ láº¡i nhiá»u láº§n nhÆ°ng váº«n lá»—i. Bá» qua dá»‹ch.")
    return ""

def batch_translate_zh_to_vi(titles, retries=3, delay=20): # TÄƒng thá»i gian chá» khi retry
    joined_titles = "\n".join(titles)
    prompt = (
        "Báº¡n lÃ  má»™t chuyÃªn gia dá»‹ch thuáº­t tiáº¿ng Trung - Viá»‡t, cÃ³ hiá»ƒu biáº¿t sÃ¢u sáº¯c vá» game mobile Trung Quá»‘c, Ä‘áº·c biá»‡t lÃ  'Nghá»‹ch Thá»§y HÃ n Mobile'.\n"
        "HÃ£y dá»‹ch táº¥t cáº£ cÃ¡c tiÃªu Ä‘á» sau sang **tiáº¿ng Viá»‡t tá»± nhiÃªn, sÃºc tÃ­ch, Ä‘Ãºng vÄƒn phong giá»›i game thá»§ Viá»‡t**, mang mÃ u sáº¯c háº¥p dáº«n, Æ°u tiÃªn giá»¯ nguyÃªn cÃ¡c thuáº­t ngá»¯ ká»¹ thuáº­t, tÃªn váº­t pháº©m, vÃ  cáº¥u trÃºc tiÃªu Ä‘á» gá»‘c.\n\n"
        "âš ï¸ Quy táº¯c dá»‹ch:\n"
        "- Giá»¯ nguyÃªn cÃ¡c cá»¥m sá»‘ (nhÆ° 10W, 288).\n"
        "- Giá»¯ nguyÃªn tÃªn ká»¹ nÄƒng, vÅ© khÃ­, tÃ­nh nÄƒng trong dáº¥u [] hoáº·c ã€ã€‘.\n"
        "- Æ¯u tiÃªn tá»« ngá»¯ phá»• biáº¿n trong cá»™ng Ä‘á»“ng game nhÆ°: 'build', 'phá»‘i Ä‘á»“', 'Ä‘áº­p Ä‘á»“', 'lá»™ trÃ¬nh', 'trang bá»‹ xá»‹n', 'ngoáº¡i hÃ¬nh Ä‘á»‰nh', 'top server'...\n"
        "- CÃ¡c tá»« cá»‘ Ä‘á»‹nh pháº£i dá»‹ch Ä‘Ãºng theo báº£ng sau:\n"
        "- æµ = lá»‘i chÆ¡i\n"
        "- æœ¨æ¡© = cá»c gá»—\n"
        "- æ²§æ¾œ = ThÆ°Æ¡ng Lan\n"
        "- æ½®å…‰ = Triá»u Quang\n"
        "- ç„æœº = Huyá»n CÆ¡\n"
        "- é¾™åŸ = Long NgÃ¢m\n"
        "- ç¥ç›¸ = Tháº§n TÆ°Æ¡ng\n"
        "- è¡€æ²³ = Huyáº¿t HÃ \n"
        "- ç¢æ¢¦ = ToÃ¡i Má»™ng\n"
        "- ç´ é—® = Tá»‘ Váº¥n\n"
        "- ä¹çµ = Cá»­u Linh\n"
        "- é“è¡£ = Thiáº¿t Y\n\n"
        "ğŸš« KhÃ´ng Ä‘Æ°á»£c thÃªm báº¥t ká»³ ghi chÃº, sá»‘ thá»© tá»±, hoáº·c pháº§n má»Ÿ Ä‘áº§u. Chá»‰ dá»‹ch tá»«ng dÃ²ng, giá»¯ nguyÃªn thá»© tá»± gá»‘c.\n\n"
        + joined_titles
    )

    headers = {"Content-Type": "application/json"}
    payload = {
        "contents": [
            {"parts": [{"text": prompt}]}
        ]
    }

    for attempt in range(retries):
        response = requests.post(API_URL, headers=headers, json=payload)
        if response.status_code == 200:
            result = response.json()
            raw_text = result["candidates"][0]["content"]["parts"][0]["text"]
            clean_text = cleanup_translation(raw_text)
            lines = [fix_terms(line.strip()) for line in clean_text.split("\n") if line.strip()]
            return lines
        elif response.status_code == 503:
            print(f"âš ï¸ MÃ´ hÃ¬nh quÃ¡ táº£i. Thá»­ láº¡i láº§n {attempt + 1}/{retries} sau {delay}s...")
            time.sleep(delay)
        else:
            print("âŒ Lá»—i dá»‹ch:", response.status_code, response.text)
            return titles

    print("âŒ Thá»­ láº¡i nhiá»u láº§n nhÆ°ng váº«n lá»—i. Bá» qua dá»‹ch.")
    return titles

def fetch_articles(url):
    print("ğŸ” Äang láº¥y dá»¯ liá»‡u tá»« album...")
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json, text/plain, */*",
        "Referer": "https://mp.weixin.qq.com/",
        "X-Requested-With": "XMLHttpRequest"
    }
    resp = requests.get(url, headers=headers)
    data = resp.json()

    articles_raw = data.get("getalbum_resp", {}).get("article_list", [])
    items = []

    weekdays_vi = [
        "Thá»© Hai", "Thá»© Ba", "Thá»© TÆ°", "Thá»© NÄƒm",
        "Thá»© SÃ¡u", "Thá»© Báº£y", "Chá»§ Nháº­t"
    ]

    for art in articles_raw:
        title = art["title"]
        url = art["url"]
        cover = art.get("cover_img_1_1") or art.get("cover") or ""
        timestamp = int(art.get("create_time", 0))
        dt = datetime.utcfromtimestamp(timestamp)
        weekday = weekdays_vi[dt.weekday()]
        date_str = f"{dt.strftime('%H:%M')} - {weekday}, {dt.strftime('%d/%m')}"

        items.append({
            "title": title,
            "url": url,
            "cover_img": cover,
            "timestamp": timestamp,
            "date": date_str
        })

    print(f"âœ… {len(items)} bÃ i viáº¿t")
    return items

def fetch_article_details(url):
    """Láº¥y ná»™i dung vÄƒn báº£n thuáº§n tÃºy, tÃ¡c giáº£ vÃ  hÃ¬nh áº£nh tá»« URL bÃ i viáº¿t."""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'en-US,en;q=0.9,vi;q=0.8',
        }
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        author_span = soup.find('span', id='js_name')
        author = author_span.text.strip() if author_span else 'KhÃ´ng rÃµ'

        content_div = soup.find('div', id='js_content')
        if not content_div:
            print("    âš ï¸ KhÃ´ng tÃ¬m tháº¥y tháº» div#js_content trong trang.")
            return None

        # TrÃ­ch xuáº¥t vÄƒn báº£n thuáº§n tÃºy, giá»¯ láº¡i dáº¥u xuá»‘ng dÃ²ng
        plain_text = content_div.get_text(separator='\n', strip=True)

        images = []
        for img_tag in content_div.find_all('img'):
            img_src = img_tag.get('data-src') or img_tag.get('src')
            if img_src:
                images.append(img_src)

        return {
            'author': author,
            'plain_text': plain_text, # Tráº£ vá» vÄƒn báº£n thuáº§n tÃºy
            'images': images
        }
    except requests.exceptions.RequestException as e:
        print(f"    âŒ Lá»—i khi táº£i chi tiáº¿t bÃ i viáº¿t: {e}")
        return None
    except Exception as e:
        print(f"    âŒ Lá»—i khÃ´ng xÃ¡c Ä‘á»‹nh khi xá»­ lÃ½ chi tiáº¿t bÃ i viáº¿t: {e}")
        return None

def fetch_all_albums(album_urls):
    all_articles = []
    for url in album_urls:
        articles = fetch_articles(url)
        top_4 = sorted(articles, key=lambda x: x["timestamp"], reverse=True)[:4]
        all_articles.extend(top_4)
    sorted_articles = sorted(all_articles, key=lambda x: x["timestamp"], reverse=True)
    return sorted_articles

# -- MAIN --
if __name__ == "__main__":
    # B1: Táº£i cache tá»« news.json náº¿u cÃ³
    existing_news = {}
    if os.path.exists("news.json"):
        try:
            with open("news.json", "r", encoding="utf-8") as f:
                old_news_list = json.load(f)
                for item in old_news_list:
                    if item.get("url"):
                        existing_news[item["url"]] = item
            print(f"âœ… ÄÃ£ táº£i {len(existing_news)} bÃ i viáº¿t tá»« cache.")
        except (json.JSONDecodeError, FileNotFoundError):
            print("âš ï¸ KhÃ´ng thá»ƒ Ä‘á»c file cache news.json, sáº½ táº¡o má»›i tá»« Ä‘áº§u.")
            existing_news = {}

    # B2: Láº¥y danh sÃ¡ch bÃ i viáº¿t má»›i nháº¥t tá»« cÃ¡c album
    articles = fetch_all_albums(ALBUMS)

    # B3: Dá»‹ch tiÃªu Ä‘á» hÃ ng loáº¡t (váº«n hiá»‡u quáº£)
    zh_titles = [a["title"] for a in articles]
    print("\nğŸŒ Äang dá»‹ch táº¥t cáº£ tiÃªu Ä‘á»...")
    vi_titles = batch_translate_zh_to_vi(zh_titles)

    # B4: Xá»­ lÃ½ tá»«ng bÃ i viáº¿t
    final_news_list = []
    for i, article_summary in enumerate(articles):
        print(f"\n[{i+1}/{len(articles)}] Äang xá»­ lÃ½: {article_summary['title']}")
        article_url = article_summary['url']

        # Kiá»ƒm tra cache: Náº¿u bÃ i viáº¿t Ä‘Ã£ cÃ³ vÃ  Ä‘Ã£ Ä‘Æ°á»£c dá»‹ch thÃ¬ dÃ¹ng láº¡i
        cached_article = existing_news.get(article_url)
        if cached_article and cached_article.get("html_content_vi"):
            print("    â¡ï¸  ÄÃ£ cÃ³ báº£n dá»‹ch trong cache, bá» qua.")
            # Cáº­p nháº­t thÃ´ng tin má»›i nháº¥t nhÆ° tiÃªu Ä‘á», ngÃ y Ä‘Äƒng
            cached_article['title_vi'] = vi_titles[i] if i < len(vi_titles) else cached_article.get('title_vi', '')
            cached_article['date'] = article_summary['date']
            final_news_list.append(cached_article)
            continue

        # Dá»‹ch tiÃªu Ä‘á»
        vi_title = vi_titles[i] if i < len(vi_titles) else article_summary["title"]
        print(f"    â¡ï¸  TiÃªu Ä‘á» VI: {vi_title}")

        # Láº¥y chi tiáº¿t bÃ i viáº¿t
        print("    â†ªï¸  Äang táº£i chi tiáº¿t bÃ i viáº¿t...")
        details = fetch_article_details(article_url)
        if not details:
            print(f"    âŒ KhÃ´ng thá»ƒ táº£i chi tiáº¿t cho: {article_summary['title']}")
            continue

        # Dá»‹ch ná»™i dung vÄƒn báº£n thuáº§n tÃºy
        print("    â†ªï¸  Äang dá»‹ch ná»™i dung vÄƒn báº£n...")
        translated_text = translate_plain_text_zh_to_vi(details['plain_text'])

        if not translated_text:
            print(f"    âŒ Dá»‹ch ná»™i dung tháº¥t báº¡i, bá» qua bÃ i viáº¿t nÃ y.")
            continue

        # Táº¡o Ä‘á»‘i tÆ°á»£ng bÃ i viáº¿t hoÃ n chá»‰nh
        full_article_data = {
            "title_zh": article_summary["title"],
            "title_vi": vi_title,
            "url": article_url,
            "cover_img": article_summary["cover_img"],
            "date": article_summary["date"],
            "author": details.get("author", "KhÃ´ng rÃµ"),
            "html_content_vi": translated_text,  # LÆ°u vÄƒn báº£n Ä‘Ã£ dá»‹ch
            "images": details.get("images", [])
        }
        final_news_list.append(full_article_data)
        print(f"    âœ… ÄÃ£ xá»­ lÃ½ xong bÃ i viáº¿t: {vi_title}")

        # TÄƒng khoáº£ng nghá»‰ giá»¯a cÃ¡c bÃ i viáº¿t Ä‘á»ƒ trÃ¡nh quÃ¡ táº£i
        time.sleep(15)

    # B5: Sáº¯p xáº¿p láº¡i danh sÃ¡ch cuá»‘i cÃ¹ng theo timestamp Ä‘á»ƒ Ä‘áº£m báº£o thá»© tá»±
    final_news_list.sort(key=lambda x: existing_news.get(x['url'], {}).get('timestamp', 0) if 'timestamp' in existing_news.get(x['url'], {}) else [a for a in articles if a['url'] == x['url']][0]['timestamp'], reverse=True)

    # B6: LÆ°u káº¿t quáº£
    with open("news.json", "w", encoding="utf-8") as f:
        json.dump(final_news_list, f, ensure_ascii=False, indent=2)

    print("\nğŸ‰ HoÃ n táº¥t! ÄÃ£ táº¡o file news.json vá»›i cÆ¡ cháº¿ cache vÃ  chá»‘ng quÃ¡ táº£i.")
